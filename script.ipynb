{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2faf49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, requests, re, html5lib, os\n",
    "from datetime import date, timedelta, datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcde386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinksPartidas(Bsoup):\n",
    "    # dado uma página de partidas \"Bsoup\" do vlr.gg, pega o link de cada partida\n",
    "    # no formato 'https://www.vlr.gg/'+links[i]\n",
    "    # detalhe que a função inverte a ordem das partidas, pra pegar da mais antiga pra mais nova\n",
    "    linksnovo_pro_antigo = []\n",
    "    links = []\n",
    "    for tag in Bsoup.find_all('a', href = True):\n",
    "        try:\n",
    "            if int(tag['href'][1]):\n",
    "                linksnovo_pro_antigo.append(tag['href'])\n",
    "        except:\n",
    "            #print('url pulada: ', tag['href'])\n",
    "            pass\n",
    "    for i in range(len(linksnovo_pro_antigo)):\n",
    "        links.append(linksnovo_pro_antigo[len(linksnovo_pro_antigo)-1-i])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfecd24d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cheguei na pag.  4\n",
      "cheguei na pag.  3\n",
      "cheguei na pag.  2\n",
      "cheguei na pag.  1\n",
      "https://www.vlr.gg/285799/paper-rex-vs-connext-roster-hunt-valorant-indonesia-connext-2023-roster-hunt\n",
      "(11, 30)\n",
      "https://www.vlr.gg/281562/zol-esports-vs-oasis-gaming-predator-league-philippines-2024-playoffs-gf\n",
      "(22, 44)\n",
      "https://www.vlr.gg/285800/connext-fantasy-1-vs-connext-fantasy-2-valorant-indonesia-connext-2023-fantasy-match\n",
      "(33, 44)\n",
      "https://www.vlr.gg/284430/full-sense-vs-made-in-thailand-esl-clash-of-nations-2023-th-closed-qualifier-lf\n",
      "(44, 58)\n",
      "https://www.vlr.gg/285801/paper-rex-vs-indonesia-pacific-valorant-indonesia-connext-2023-vct-pacific-showdown\n",
      "(55, 58)\n",
      "https://www.vlr.gg/285295/mouz-vs-alternate-attax-valorant-challengers-2023-dach-arcade-playoffs-ubsf\n",
      "(66, 72)\n",
      "https://www.vlr.gg/281031/cloud9-vs-furia-superdome-2023-colombia-lbf\n",
      "(77, 72)\n",
      "https://www.vlr.gg/281029/leviat-n-vs-furia-superdome-2023-colombia-gf\n",
      "(88, 72)\n"
     ]
    }
   ],
   "source": [
    "def encontrar_data(Bsoup, data_pegar, Links):\n",
    "    for dia in Bsoup('div', class_='wf-label mod-large'):\n",
    "        data = dia.text.replace(' ', '').replace('\\n', '').replace('\\t', '')\n",
    "        data = datetime.strptime(data, '%a,%B%d,%Y').date()\n",
    "\n",
    "        if data >= data_pegar:\n",
    "            return 1, 0\n",
    "\n",
    "    return 0, len(Links)\n",
    "    \n",
    "\n",
    "\n",
    "def salvar_csv(data_pegar):\n",
    "    df = pd.DataFrame()\n",
    "    url = 'https://www.vlr.gg/matches/results/'\n",
    "    url_pag = 'https://www.vlr.gg/matches/results/?page='\n",
    "\n",
    "    # # garante que pega todas as partidas de ontem, com folga\n",
    "    pagina_inicial = 4\n",
    "    nome_col = 0\n",
    "    passei_da_data=0\n",
    "\n",
    "    for k in range(pagina_inicial, 0, -1): \n",
    "        if passei_da_data:\n",
    "            break\n",
    "        print('cheguei na pag. ', str(k))\n",
    "        matches_page = requests.get(url_pag + str(k))\n",
    "        Bsoup = BeautifulSoup(matches_page.text, 'html.parser')\n",
    "        Links = LinksPartidas(Bsoup)\n",
    "        m = 0\n",
    "        comeco = 0\n",
    "        while m < len(Links):\n",
    "            if comeco == 0:\n",
    "                comeco, m = encontrar_data(Bsoup, data_pegar, Links)\n",
    "                \n",
    "            variavel = m\n",
    "            \n",
    "            for h in range(variavel,len(Links)):\n",
    "                match_url = 'https://www.vlr.gg' + Links[h]\n",
    "                match_soup = BeautifulSoup(requests.get(match_url).text, 'html.parser')\n",
    "                data_partida = (datetime.strptime(\n",
    "                                match_soup('div', class_ = 'moment-tz-convert')[0]\n",
    "                                .get('data-utc-ts'), \"%Y-%m-%d %H:%M:%S\")\n",
    "                                .date())\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    if (match_soup('div', class_ = 'wf-title-med')[0].text.find('TBD') == -1 and\n",
    "                            match_soup('div', class_ = 'wf-title-med')[1].text.find('TBD') == -1 and\n",
    "                            match_soup('div', class_ = 'vm-stats-container')[0].text.find('No data available for this match') == -1 and\n",
    "                            len(match_soup('tbody')[2]('tr')[0]('td')[1]('div')[0].contents) > 1 and\n",
    "                            data_partida >= data_pegar):\n",
    "                            #and\n",
    "                            #match_soup('tbody')[2]('tr')[0]('td')[3]('span')[1].text != ' ' and\n",
    "                            #match_soup('tbody')[3]('tr')[0]('td')[3]('span')[1].text != ' '):\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "                m += 1\n",
    "            \n",
    "            \n",
    "            if m >= len(Links):\n",
    "                break\n",
    "            \n",
    "            data_partida = (datetime.strptime(\n",
    "                                match_soup('div', class_ = 'moment-tz-convert')[0]\n",
    "                                .get('data-utc-ts'), \"%Y-%m-%d %H:%M:%S\")\n",
    "                                .date())\n",
    "\n",
    "            if data_partida > data_pegar:\n",
    "                passei_da_data=1\n",
    "                print(str(data_pegar), 'completo')\n",
    "                break\n",
    "\n",
    "            print(match_url)\n",
    "            panda = pd.read_html(match_url)        \n",
    "            temp = pd.concat([panda[2], panda[3]], ignore_index=True)\n",
    "            \n",
    "            try:\n",
    "                temp2 = {'scores_team_1':[int(match_soup('div', class_ = 'js-spoiler')[0]('span')[0].text)] ,\n",
    "                        'scores_team_2':[int(match_soup('div', class_ = 'js-spoiler')[0]('span')[2].text)]}\n",
    "            except:\n",
    "                temp2 = {'scores_team_1' : [], 'scores_team_2' : []}\n",
    "                print('erro placar pag: ', k)\n",
    "                pass\n",
    "            for i in range(0, len(panda)-2, 2):\n",
    "                temp2['scores_team_1'].append(int(match_soup('div', class_ = 'score')[i].text))\n",
    "                temp2['scores_team_2'].append(int(match_soup('div', class_ = 'score')[i+1].text))\n",
    "                \n",
    "            temp2 = pd.DataFrame(temp2)\n",
    "            temp = pd.concat([temp, temp2], axis = 1)\n",
    "            \n",
    "            if nome_col == 0:\n",
    "                colunas_v = panda[2].columns.tolist()\n",
    "                colunas_n = [[],[],[],[],[]]\n",
    "                for coluna in colunas_v:\n",
    "                    colunas_n[0].append('Map 1'+coluna)\n",
    "                    colunas_n[1].append('Map 2'+coluna)\n",
    "                    colunas_n[2].append('Map 3'+coluna)\n",
    "                    colunas_n[3].append('Map 4'+coluna)\n",
    "                    colunas_n[4].append('Map 5'+coluna)\n",
    "                nome_col += 1\n",
    "                \n",
    "            panda[0].rename(columns = dict(zip(colunas_v, colunas_n[0])), inplace = True)\n",
    "            panda[1].rename(columns = dict(zip(colunas_v, colunas_n[0])), inplace = True)\n",
    "            \n",
    "            temp = pd.concat([temp, pd.concat([panda[0], panda[1]], ignore_index=True)], axis = 1)\n",
    "            for j in range(4, len(panda), 2):\n",
    "                panda[j].rename(columns = dict(zip(colunas_v, colunas_n[j//2-1])), inplace = True)\n",
    "                panda[j+1].rename(columns = dict(zip(colunas_v, colunas_n[j//2-1])), inplace = True)\n",
    "                temp = pd.concat([temp, pd.concat([panda[j], panda[j+1]], ignore_index=True)], axis = 1)\n",
    "            \n",
    "            df = pd.concat([df, temp], ignore_index = True)\n",
    "            df = pd.concat([df, pd.DataFrame(np.full(df.shape[1], np.nan)[None], columns = df.columns)])\n",
    "            print(df.shape)\n",
    "\n",
    "            m += 1\n",
    "        \n",
    "    df.to_csv(f'jogos_por_dia\\\\{str(data_pegar)}.csv')\n",
    "\n",
    "    return\n",
    "            \n",
    "# # pegar partidas desta data, ou seja, ontem\n",
    "data_pegar = date.today() - timedelta(days = 1)\n",
    "\n",
    "# # olhar pra ultima partida que esse script pegou\n",
    "proxima_data = datetime.strptime(os.listdir('jogos_por_dia')[-1][:-4], '%Y-%m-%d').date() + timedelta(days=1)\n",
    "\n",
    "if proxima_data == data_pegar:\n",
    "    salvar_csv(data_pegar)\n",
    "elif proxima_data < data_pegar:\n",
    "    dias = data_pegar - proxima_data\n",
    "    while dias != timedelta(days=0):\n",
    "        salvar_csv(proxima_data)\n",
    "        proxima_data+= timedelta(days=1)\n",
    "        dias-= timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
